/*
 * Copyright (c) 2014-2016 Snowplow Analytics Ltd. All rights reserved.
 *
 * This program is licensed to you under the Apache License Version 2.0,
 * and you may not use this file except in compliance with the Apache License Version 2.0.
 * You may obtain a copy of the Apache License Version 2.0 at http://www.apache.org/licenses/LICENSE-2.0.
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the Apache License Version 2.0 is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the Apache License Version 2.0 for the specific language governing permissions and limitations there under.
 */
package com.snowplowanalytics.iglu.schemaddl
package generators
package redshift

// Scalaz
import scalaz._
import Scalaz._

// Java
import java.text.SimpleDateFormat
import java.util.Calendar

// Scala
import scala.annotation.tailrec
import scala.collection.immutable.ListMap

// This project
import utils.{ StringUtils => SU }
import SchemaData._

/**
 * Generates a Redshift DDL File from a Flattened JsonSchema
 */
object RedshiftDdlGenerator {
  import redshift.Ddl._
  import redshift.TypeSuggestions._
  import redshift.EncodeSuggestions._

  // Settings for the header of the file
  private object HeaderTextSettings {
    val name = generated.ProjectSettings.name
    val version = generated.ProjectSettings.version
    val datetime = new SimpleDateFormat("YYYY-MM-dd HH:mm").format(Calendar.getInstance.getTime)
  }

  // Header Section for a Redshift DDL File
  val RedshiftDdlHeader =
    s"""|-- AUTO-GENERATED BY ${HeaderTextSettings.name} DO NOT EDIT
        |-- Generator: ${HeaderTextSettings.name} ${HeaderTextSettings.version}
        |-- Generated: ${HeaderTextSettings.datetime}""".stripMargin

  /**
   * Make a DDL header from the self-describing info
   *
   * @param flatSelfElems self-describing info
   * @param schemaName optional schema name
   * @return SQL comment
   */
  def getTableComment(tableName: String, schemaName: Option[String], flatSelfElems: SelfDescInfo): Comment = {
    val schema = schemaName.map(_ + ".").getOrElse("")
    Ddl.Comment(schema + tableName, SU.getSchemaName(flatSelfElems))
  }

  /**
   * Make a DDL header from the file name
   *
   * @param fileName JSON Schema file name
   * @param schemaName optional schema name
   * @return SQL comment
   */
  def getTableComment(tableName: String, schemaName: Option[String], fileName: String): Comment = {
    val schema = schemaName.map(_ + ".").getOrElse("")
    Ddl.Comment(schema + tableName, "Source: " + fileName)
  }

  // Columns with data taken from self-describing schema
  private[redshift] val selfDescSchemaColumns = List(
    Column("schema_vendor", DataTypes.RedshiftVarchar(128), Set(CompressionEncodings.RunLengthEncoding), Set(ColumnConstraints.NotNull)),
    Column("schema_name", DataTypes.RedshiftVarchar(128), Set(CompressionEncodings.RunLengthEncoding), Set(ColumnConstraints.NotNull)),
    Column("schema_format", DataTypes.RedshiftVarchar(128), Set(CompressionEncodings.RunLengthEncoding), Set(ColumnConstraints.NotNull)),
    Column("schema_version", DataTypes.RedshiftVarchar(128), Set(CompressionEncodings.RunLengthEncoding), Set(ColumnConstraints.NotNull))
  )

  // Snowplow-specific columns
  private[redshift] val parentageColumns = List(
    Column("root_id", DataTypes.RedshiftChar(36), Set(CompressionEncodings.RawEncoding), Set(ColumnConstraints.NotNull)),
    Column("root_tstamp", DataTypes.RedshiftTimestamp, Set(CompressionEncodings.LzoEncoding), Set(ColumnConstraints.NotNull)),
    Column("ref_root", DataTypes.RedshiftVarchar(255), Set(CompressionEncodings.RunLengthEncoding), Set(ColumnConstraints.NotNull)),
    Column("ref_tree", DataTypes.RedshiftVarchar(1500), Set(CompressionEncodings.RunLengthEncoding), Set(ColumnConstraints.NotNull)),
    Column("ref_parent", DataTypes.RedshiftVarchar(255), Set(CompressionEncodings.RunLengthEncoding), Set(ColumnConstraints.NotNull))
  )

  /**
   * Generates Redshift Table object with all columns, attributes and constraints
   *
   * @param flatSchema flat schema produced by the Schema flattening process
   * @param schemaName optional redshift schema name
   * @param rawMode do not produce any Snowplow specific columns (like root_id)
   * @param size default length for VARCHAR
   * @return Table object with all data about table creation
   */
  def getTableDdl(flatSchema: FlatSchema, tableName: String, schemaName: Option[String], size: Int, rawMode: Boolean = false): Table = {
    val columns = getColumnsDdl(flatSchema.elems, flatSchema.required, size)
                    .toList
//                    .map(c => c.copy(columnName = SU.snakify(c.columnName)))
                    .sortBy(c => (-c.columnConstraints.size, c.columnName))
    if (rawMode) {
      val fullTableName = schemaName match {
        case Some(schema) => schema + "." + tableName
        case None => tableName
      }
      Table(fullTableName, columns)
    } else {
      val schema = if (schemaName.isDefined) schemaName.get else "atomic"
      val fullTableName =  schema + "." + tableName
      val tableConstraints = Set[TableConstraint](RedshiftDdlDefaultForeignKey(schema))
      val tableAttributes = Set[TableAttribute]( // Snowplow-specific attributes
        TableAttributes.Diststyle(TableAttributes.Key),
        TableAttributes.Distkey("root_id"),
        TableAttributes.Sortkey(None, NonEmptyList("root_tstamp"))
      )
      Table(fullTableName, selfDescSchemaColumns ++ parentageColumns ++ columns, tableConstraints, tableAttributes)
    }
  }

  /**
   * Get DDL for Foreign Key for specified schema
   *
   * @param schemaName Redshift's schema
   * @return ForeignKey constraint
   */
  private def RedshiftDdlDefaultForeignKey(schemaName: String) = {
    val reftable = RefTable(schemaName + ".events", Some("event_id"))
    TableConstraints.ForeignKey(NonEmptyList("root_id"), reftable)
  }

  /**
   * Processes the Map of Data elements pulled from the JsonSchema and
   * generates DDL object for it with it's name, constrains, attributes
   * data type, etc
   *
   * @param flatDataElems The Map of Schema keys -> attributes which need to
   *                      be processed
   * @param required required fields to decide which columns are nullable
   * @return a list of Column DDLs
   */
  private[generators] def getColumnsDdl(flatDataElems: ListMap[String, Map[String, String]], required: Set[String], varcharSize: Int): Iterable[Column] = {
    // Process each key pair in the map
    for {
      (columnName, properties) <- flatDataElems
    } yield {
      val dataType = getDataType(properties, varcharSize, columnName)
      val encoding = getEncoding(properties, dataType, columnName)
      val constraints =    // only "NOT NULL" now
        if (checkNullability(properties, required.contains(columnName))) Set.empty[ColumnConstraint]
        else Set[ColumnConstraint](ColumnConstraints.NotNull)
      Column(columnName, dataType, columnAttributes = Set(encoding), columnConstraints = constraints)
    }
  }

  // List of data type suggestions
  val dataTypeSuggestions: List[DataTypeSuggestion] = List(
    complexEnumSuggestion,
    productSuggestion,
    timestampSuggestion,
    arraySuggestion,
    integerSuggestion,
    numberSuggestion,
    booleanSuggestion,
    charSuggestion,
    uuidSuggestion,
    varcharSuggestion
  )

  // List of compression encoding suggestions
  val encodingSuggestions: List[EncodingSuggestion] = List(lzoSuggestion)


  /**
   * Takes each suggestion out of ``dataTypeSuggesions`` and decide whether
   * current properties satisfy it, then return the data type
   * If nothing suggested VARCHAR with ``varcharSize`` returned as default
   *
   * @param properties is a string we need to recognize
   * @param varcharSize default size for unhandled properties and strings
   *                    without any data about length
   * @param columnName to produce warning
   * @param suggestions list of functions can recognize encode type
   * @return some format or none if nothing suites
   */
  @tailrec
  private[generators] def getDataType(properties: Map[String, String],
                                      varcharSize: Int,
                                      columnName: String,
                                      suggestions: List[DataTypeSuggestion] = dataTypeSuggestions): DataType = {
    suggestions match {
      case Nil => DataTypes.RedshiftVarchar(varcharSize)    // Generic
      case suggestion :: tail => suggestion(properties, columnName) match {
        case Some(format) => format
        case None => getDataType(properties, varcharSize, columnName, tail)
      }
    }
  }

  /**
   * Takes each suggestion out of ``compressionEncodingSuggestions`` and
   * decide whether current properties satisfy it, then return the compression
   * encoding.
   * If nothing suggested LZO Encoding returned as default
   *
   * @param properties is a string we need to recognize
   * @param dataType redshift data type for current column
   * @param columnName to produce warning
   * @param suggestions list of functions can recognize encode type
   * @return some format or none if nothing suites
   */
  @tailrec
  private[generators] def getEncoding(properties: Map[String, String],
                                      dataType: DataType,
                                      columnName: String,
                                      suggestions: List[EncodingSuggestion] = encodingSuggestions): CompressionEncoding = {
    suggestions match {
      case Nil => CompressionEncodings.LzoEncoding    // LZO is default for user-generated
      case suggestion :: tail => suggestion(properties, dataType, columnName) match {
        case Some(encoding) => encoding
        case None => getEncoding(properties, dataType, columnName, tail)
      }
    }
  }

  /**
   * Check whether field can be null.
   * Priority of factors:
   * - "null" in type
   * - null in enum
   * - property is in required array
   *
   * @param properties hash map of JSON Schema properties for primitive type
   * @param required whether this field listed in required array
   * @return nullable or not
   */
  private[generators] def checkNullability(properties: Map[String, String], required: Boolean): Boolean = {
    (properties.get("type"), properties.get("enum")) match {
      case (Some(types), _) if types.contains("null") => true
      case (_, Some(enum)) if enum.split(",").toList.contains("null") => true
      case _ => !required
    }
  }
}
